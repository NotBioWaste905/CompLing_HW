{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2ch_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    dvach = f.readlines()\n",
    "    test_corpora = random.sample(dvach, 50)\n",
    "\n",
    "    for i in test_corpora:\n",
    "        dvach.remove(i)\n",
    "\n",
    "    dvach = '\\n'.join(dvach)\n",
    "\n",
    "\n",
    "\n",
    "with open('lenta.txt', 'r', encoding='utf-8') as f:\n",
    "    news = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0afceb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина 1 - 11719282\n",
      "Длина 2 - 11536552\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина 1 -\", len(dvach))\n",
    "print(\"Длина 2 -\", len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d429c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db3615db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина корпуса токсичных постов в токенах - 1858107\n",
      "Длина корпуса новостных текстов в токенах -  1505789\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина корпуса токсичных постов в токенах -\", len(norm_dvach))\n",
    "print(\"Длина корпуса новостных текстов в токенах - \", len(norm_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d19dded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3d0e958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 55861),\n",
       " ('в', 48834),\n",
       " ('не', 46577),\n",
       " ('на', 29647),\n",
       " ('что', 26649),\n",
       " ('я', 21724),\n",
       " ('а', 21302),\n",
       " ('с', 21071),\n",
       " ('это', 17721),\n",
       " ('ты', 15459)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dvach.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58b88e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 72412),\n",
       " ('и', 33290),\n",
       " ('на', 28434),\n",
       " ('по', 19490),\n",
       " ('что', 17031),\n",
       " ('с', 15921),\n",
       " ('не', 12702),\n",
       " ('из', 7727),\n",
       " ('о', 7515),\n",
       " ('как', 7514)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_news.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb103968",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "884fcf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    for word in normalize(text):\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += (np.log(1/len(norm_dvach)))\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c1e0f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст похож на новостные тексты\n"
     ]
    }
   ],
   "source": [
    "phrase = \"владимир путин подписал указ о введении новых налогов на богатых россиян\"\n",
    "if compute_joint_proba(phrase, probas_dvach) > compute_joint_proba(phrase, probas_news):\n",
    "    print(\"Текст похож на токсичные посты\")\n",
    "else:\n",
    "    print(\"Текст похож на новостные тексты\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7293f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "sentences_dvach = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach[:5000000])]\n",
    "sentences_news = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64f10515",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "\n",
    "trigrams_dvach = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    trigrams_dvach.update(ngrammer(sentence, 3))\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    trigrams_news.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b2c6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba_markov_assumption(text, word_counts, bigram_counts):\n",
    "    prob = 0\n",
    "    for ngram in ngrammer(['<start>'] + normalize(text) + ['<end>']):\n",
    "        word1, word2 = ngram.split()\n",
    "        if word1 in word_counts and ngram in bigram_counts:\n",
    "            prob += np.log(bigram_counts[ngram]/word_counts[word1])\n",
    "        # small value for unk words\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return np.exp(prob)\n",
    "\n",
    "def compute_joint_proba_trigram(text, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = ['<start>', '<start>'] + normalize(text) + ['<end>']\n",
    "    for ngram in ngrammer(tokens, n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = f\"{word1} {word2}\"\n",
    "        trigram = f\"{word1} {word2} {word3}\"\n",
    "        if bigram in bigram_counts and trigram in trigram_counts and bigram_counts[bigram] > 0:\n",
    "            prob += np.log(trigram_counts[trigram] / bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35d62f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст похож на новостные тексты\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Технические возможности бесполезного российского судна не позволили разгрузить его у терминала\"\n",
    "if compute_joint_proba_trigram(phrase, bigrams_dvach, trigrams_dvach) > compute_joint_proba_trigram(phrase, bigrams_news, trigrams_news):\n",
    "    print(\"Текст похож на токсичные посты\")\n",
    "else:\n",
    "    print(\"Текст похож на новостные тексты\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9b8e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110835\n",
      "494948\n",
      "692060\n"
     ]
    }
   ],
   "source": [
    "# матрица слова на слова (инициализируем нулями)\n",
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                          len(unigrams_dvach)))\n",
    "\n",
    "# к матрице нужно обращаться по индексам\n",
    "# поэтому зафиксируем порядок слов в словаре и сделаем маппинг id-слово и слово-id\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "print(len(id2word_dvach))\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "print(len(id2bigram_dvach))\n",
    "bigram2id_dvach = {ngram:i for i, ngram in enumerate(id2bigram_dvach)}\n",
    "\n",
    "id2trigram_dvach = list(trigrams_dvach)\n",
    "print(len(id2trigram_dvach))\n",
    "trigram2id_dvach = {ngram:i for i, ngram in enumerate(id2trigram_dvach)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa04d2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> <start>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2bigram_dvach[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1d98864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполняем матрицу\n",
    "for ngram in trigrams_dvach:\n",
    "    bigram, word2 = \" \".join(ngram.split()[:2]), ngram.split()[2]\n",
    "    try:\n",
    "        # на пересечение двух слов ставим вероятность встретить второе после первого\n",
    "        matrix_dvach[bigram2id_dvach[bigram], word2id_dvach[word2]] =  (trigrams_dvach[ngram]/\n",
    "                                                                     bigrams_dvach[bigram])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ngram \\\"{ngram}\\\": {e}\")\n",
    "matrix_dvach = csc_matrix(matrix_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85d63a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, id2bigram, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        probs = matrix[current_idx].toarray()[0]\n",
    "        probs_sum = probs.sum()\n",
    "        if probs_sum == 0:\n",
    "            # fallback: end the sentence\n",
    "            text.append('<end>')\n",
    "            break\n",
    "        probs = probs / probs_sum  # normalize\n",
    "        chosen = np.random.choice(matrix.shape[1], p=probs)\n",
    "        text.append(id2word[chosen])\n",
    "        if id2word[chosen] == '<end>':\n",
    "            break\n",
    "        # update current_idx to the new bigram\n",
    "        prev_bigram = id2bigram[current_idx].split()\n",
    "        new_bigram = f\"{prev_bigram[1]} {id2word[chosen]}\"\n",
    "        current_idx = bigram2id.get(new_bigram, bigram2id['<start> <start>'])\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "096e21b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "блжад с базой данных пользователей \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, word2id_dvach, id2bigram_dvach, bigram2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "780be3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)\n",
    "\n",
    "def apply_temperature(probas, temperature):\n",
    "    # логарифмирование и деление на температуру\n",
    "    log_probas = np.log(np.maximum(probas, 1e-10))  \n",
    "    adjusted_log_probas = log_probas / temperature\n",
    "    # чтобы получить честные вероятности, нужно применить софтмакс\n",
    "    exp_probas = np.exp(adjusted_log_probas)\n",
    "    adjusted_probabilities = exp_probas / np.sum(exp_probas)\n",
    "    return adjusted_probabilities\n",
    "\n",
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for word in tokens:\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "def compute_joint_proba_triplet(text, word_probas, bigram_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for i in range(len(tokens) - 2):\n",
    "        word1, word2, word3 = tokens[i], tokens[i + 1], tokens[i + 2]\n",
    "        bigram = f\"{word1} {word2}\"\n",
    "        if bigram in bigram_probas and word3 in word_probas:\n",
    "            prob += np.log(bigram_probas[bigram]) + np.log(word_probas[word3])\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb191a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(17732.353112215213)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Безграмотное быдло с дубляжом, войсовером, порнографией и котикам'\n",
    "perplexity(*compute_joint_proba(phrase, probas_dvach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a78ef8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3951.4400591275294)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba_triplet(phrase, probas_dvach, bigrams_dvach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "870a4772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3168.1353134089277)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = []\n",
    "for sent in sent_tokenize('\\n'.join(test_corpora)):\n",
    "    prob, N = compute_joint_proba_triplet(sent, probas_dvach, bigrams_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))\n",
    "\n",
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
